<HTML>
<HEAD>
<TITLE>Glynx - a download manager.</TITLE>
<LINK REV="made" HREF="mailto:root@porky.devel.redhat.com">
</HEAD>

<BODY>

<!-- INDEX BEGIN -->

<UL>

	<LI><A HREF="#NAME">NAME</A>
	<LI><A HREF="#DESCRIPTION">DESCRIPTION</A>
	<LI><A HREF="#SYNOPSIS">SYNOPSIS</A>
	<LI><A HREF="#HINTS">HINTS</A>
	<LI><A HREF="#COMMAND_LINE_OPTIONS">COMMAND-LINE OPTIONS</A>
	<LI><A HREF="#COPYRIGHT">COPYRIGHT</A>
</UL>
<!-- INDEX END -->

<HR>
<P>
<H1><A NAME="NAME">NAME</A></H1>
<P>
Glynx - a download manager. 

<P>
<HR>
<H1><A NAME="DESCRIPTION">DESCRIPTION</A></H1>
<P>
Glynx makes a local image of a selected part of the internet.

<P>
It can be used to make download lists to be used with other download
managers, making a distributed download process.

<P>
It currently supports resume/retry, referer, user-agent, frames,
distributed download (see <CODE>--slave</CODE>, <CODE>--stop</CODE>, <CODE>--restart</CODE>).

<P>
It partially supports: redirect (using file-copy), java, javascript,
multimedia, authentication (only basic), mirror, translating links to local
computer (<CODE>--makerel</CODE>), correcting file extensions, ftp, renaming too long filenames and too
deep directories, cookies, proxy, forms.

<P>
A very basic cgi user interface is included.

<P>
No testing so far: ``https:''.

<P>
Tested on Linux and NT

<P>
<HR>
<H1><A NAME="SYNOPSIS">SYNOPSIS</A></H1>
<DL>
<DT><STRONG><A NAME="item_Do">Do-everything at once:</A></STRONG><DD>
<P>
<PRE> glynx.pl [options] &lt;URL&gt;
</PRE>
<DT><STRONG><A NAME="item_Save">Save work to finish later:</A></STRONG><DD>
<P>
<PRE> glynx.pl [options] --dump=&quot;dump-file&quot; &lt;URL&gt;
</PRE>
<DT><STRONG><A NAME="item_Finish">Finish saved download:</A></STRONG><DD>
<P>
<PRE> glynx.pl [options] &quot;download-list-file&quot;
</PRE>
<DT><STRONG><A NAME="item_Network">Network mode (client/slave)</A></STRONG><DD>
<DT><STRONG><A NAME="item__">- Clients:</A></STRONG><DD>
<P>
<PRE> glynx.pl [options] --dump=&quot;dump-file&quot; &lt;URL&gt;
</PRE>
<DT><STRONG>- Slaves (will wait until there is something to do):</STRONG><DD>
<P>
<PRE> glynx.pl [options] --slave
</PRE>
</DL>
<P>
<HR>
<H1><A NAME="HINTS">HINTS</A></H1>
<P>
How to create a default configuration:

<P>
<PRE>        Start the program with all command-line configurations, plus --cfg-save
        or:
        1 - start the program with --cfg-save
        2 - edit glynx.ini file
</PRE>
<P>
--subst, --exclude and --loop use regular expressions.

<P>
<PRE>   <A HREF="http://www.site.com/old.htm">http://www.site.com/old.htm</A> --subst=s/old/new/
   downloads: <A HREF="http://www.acme.com/new.htm">http://www.acme.com/new.htm</A>
</PRE>
<P>
<PRE>   - Note: the substitution string MUST be made of &quot;valid URL&quot; characters
</PRE>
<P>
<PRE>   --exclude=/\.gif/
   will not download &quot;.gif&quot; files
</PRE>
<P>
<PRE>   - Note: Multiple --exclude are allowed:
</PRE>
<P>
<PRE>   --exclude=/gif/  --exclude=/jpeg/
   will not download &quot;.gif&quot; or &quot;.jpeg&quot; files
</PRE>
<P>
<PRE>   It can also be written as:
   --exclude=/\.gif|\.jp.?g/i
   matching .gif, .GIF, .jpg, .jpeg, .JPG, .JPEG
</PRE>
<P>
<PRE>   --exclude=/www\.site\.com/
   will not download links containing the site name
</PRE>
<P>
<PRE>   <A HREF="http://www.site.com/bin/index.htm">http://www.site.com/bin/index.htm</A> --prefix=<A HREF="http://www.site.com/bin/">http://www.site.com/bin/</A>
   won't download outside from directory &quot;/bin&quot;. Prefix must end with a slash &quot;/&quot;.
</PRE>
<P>
<PRE>   <A HREF="http://www.site.com/index%%%.htm">http://www.site.com/index%%%.htm</A> --loop=%%%:0..3
   will download:
     <A HREF="http://www.site.com/index0.htm">http://www.site.com/index0.htm</A>
     <A HREF="http://www.site.com/index1.htm">http://www.site.com/index1.htm</A>
     <A HREF="http://www.site.com/index2.htm">http://www.site.com/index2.htm</A>
     <A HREF="http://www.site.com/index3.htm">http://www.site.com/index3.htm</A>
</PRE>
<P>
<PRE>   - Note: the substitution string MUST be made of &quot;valid URL&quot; characters
</PRE>
<P>
- For multiple exclusion: use ``|''.

<P>
- Don't read directory-index:

<P>
<PRE>        ?D=D ?D=A ?S=D ?S=A ?M=D ?M=A ?N=D ?N=A =&gt;  \?[DSMN]=[AD] 
</PRE>
<P>
<PRE>        To change default &quot;exclude&quot; pattern - put it in the configuration file
</PRE>
<P>
Note: ``File:'' item in dump file is ignored

<P>
You can filter the processing of a dump file using --prefix, --exclude,
--subst

<P>
If after finishing downloading you still have ``.PART._BUSY_'' files in the
base directory, rename them to ``.PART'' (the program should do this by
itself)

<P>
Don't do this: --depth=1 --out-depth=3 because ``out-depth'' is an upper
limit; it is tested after depth is generated. The right way is: --depth=4
--out-depth=3

<P>
This will do nothing:

<P>
<PRE> --dump=x graphic.gif
</PRE>
<P>
because the dump file gets all binary files.

<P>
Errors using https:

<P>
<PRE> [ ERROR 501 Protocol scheme 'https' is not supported =&gt; LATER ] or
 [ ERROR 501 Can't locate object method &quot;new&quot; via package &quot;LWP::Protocol::https&quot; =&gt; LATER ]
</PRE>
<P>
This means you need to install at least ``openssl''
(http://www.openssl.org), Net::SSLeay and IO::Socket::SSL

<P>
<HR>
<H1><A NAME="COMMAND_LINE_OPTIONS">COMMAND-LINE OPTIONS</A></H1>
<P>
Check --help for default values.

<P>
Very basic:

<P>
<PRE>  --version         Print version number and quit
  --verbose         More output
  --quiet           No output
  --help            Help page
  --cfg-save        Save configuration to file
  --base-dir=DIR    Place to load/save files
</PRE>
<P>
Download options are:

<P>
<PRE>  --sleep=SECS      Sleep between gets, ie. go slowly
  --prefix=PREFIX   Limit URLs to those which begin with PREFIX
                    Multiple &quot;--prefix&quot; are allowed.
  --depth=N         Maximum depth to traverse
  --out-depth=N     Maximum depth to traverse outside of PREFIX
  --referer=URI     Set initial referer header
  --limit=N         A limit on the number documents to get
  --retry=N         Maximum number of retrys
  --timeout=SECS    Timeout value - increases on retrys
  --agent=AGENT     User agent name
  --mirror          Checks all existing files for updates
  --nomirror        Do not check for updates -- if file exists, it's ok
  --mediaext        Creates a file link, guessing the media type extension (.jpg, .gif)
                    (perl actually makes a file copy)
  --nomediaext      Do not try to change media type extension
  --makerel         Make Relative links. Links in pages will work in the
                    local computer.
  --nomakerel       Keep links as they are. Do not try to change links.
  --auth=USER:PASS  Set authentication credentials
  --cookies=FILE    Set up a cookies file (default is no cookies)
  --name-len-max    Limit filename size
  --dir-depth-max   Limit directory depth
</PRE>
<P>
Multi-process control:

<P>
<PRE>  --slave           Wait until a download-list file is created (be a slave)
  --stop            Stop slave
  --restart         Stop and restart slave
</PRE>
<P>
Not implemented yet but won't generate fatal errors (compatibility with
lwp-rget):

<P>
<PRE>  --hier            Download into hierarchy (not all files into cwd)
  --iis             Workaround IIS 2.0 bug by sending &quot;Accept: */*&quot; MIME
                    header; translates backslashes (\) to forward slashes (/)
  --keepext=type    Keep file extension for MIME types (comma-separated list)
  --nospace         Translate spaces URLs (not #fragments) to underscores (_)
  --tolower         Translate all URLs to lowercase (useful with IIS servers)
</PRE>
<P>
Other options: (to-be better explained)

<P>
<PRE>  --indexfile=FILE  Index file in a directory
  --part-suffix=.SUFFIX  Extension to use for partial downloads 
                    (example: &quot;.Getright&quot; &quot;.PART&quot;)
  --dump=FILE       make download-list file, to be used later
  --dump-max=N      number of links per download-list file 
  --invalid-char=C  Character to use in substitutions for invalid characters
  --exclude=/REGEXP/i  Don't download matching URLs
                    Multiple --exclude are allowed
  --loop=REGEXP:INITIAL..FINAL  Expand a URL through substitutions 
                    (example: xx:a,b,c  xx:'01'..'10')
  --subst=s/REGEXP/VALUE/i  Substitute some string in the urls.
  --404-retry       will retry on error 404 Not Found. 
  --no404-retry     creates an empty file on error 404 Not Found.
</PRE>
<P>
<HR>
<H1><A NAME="COPYRIGHT">COPYRIGHT</A></H1>
<P>
Copyright (c) 2000 Flavio Glock &lt;<A
HREF="mailto:fglock@pucrs.br">fglock@pucrs.br</A>&gt; All rights reserved.
This program is free software; you can redistribute it and/or modify it
under the same terms as Perl itself. This program was based on examples in
the Perl distribution.

<P>
If you use it/like it, send a postcard to the author. 

</BODY>

</HTML>
